{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/premrao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing almost every file we have learned thus far, if used in the project or not.\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Function to Scrape data from the reddit website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redditscraper(url, number_of_posts):\n",
    "\n",
    "    all_posts =[]\n",
    "    p = {}\n",
    "        #using a for loop to iterate through the number of posts\n",
    "    for _ in range(number_of_posts): \n",
    "        \n",
    "\n",
    "        # Get the posts by hitting the url, put it in json and store it\n",
    "        res = requests.get(url,params=p, headers={'User-agent': 'premrao'})\n",
    "        data = res.json()\n",
    "        res.raise_for_status()\n",
    "        list_of_posts = data['data']['children']\n",
    "        all_posts = all_posts + list_of_posts\n",
    "\n",
    "        # reassign the after to the current 'after', and then update the url to hit\n",
    "        after = data['data']['after']\n",
    "        if after == None:\n",
    "            print(\"Print 'after' recieved: \" , after)\n",
    "            break\n",
    "        else:\n",
    "            p.update({'after':after})\n",
    "            print(\"Print 'after' recieved: \", after)\n",
    "            \n",
    "        # go to sleep for half a second so you do not overwhelm reddit servers or cause an alert on their system\n",
    "      \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    return all_posts\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print 'after' recieved:  t3_9exrkc\n",
      "Print 'after' recieved:  t3_9eymsc\n",
      "Print 'after' recieved:  t3_9exdyc\n",
      "Print 'after' recieved:  t3_9eo4by\n",
      "Print 'after' recieved:  t3_9ewhv9\n",
      "Print 'after' recieved:  t3_9ep9ym\n",
      "Print 'after' recieved:  t3_9epqq4\n",
      "Print 'after' recieved:  t3_9engg5\n",
      "Print 'after' recieved:  t3_9ej6s7\n",
      "Print 'after' recieved:  t3_9ehgt6\n",
      "Print 'after' recieved:  t3_9ej5d9\n",
      "Print 'after' recieved:  t3_9ehs0k\n",
      "Print 'after' recieved:  t3_9e4245\n",
      "Print 'after' recieved:  t3_9eair6\n",
      "Print 'after' recieved:  t3_9e5hty\n",
      "Print 'after' recieved:  t3_9e27gg\n",
      "Print 'after' recieved:  t3_9dwokl\n",
      "Print 'after' recieved:  t3_9dyxda\n",
      "Print 'after' recieved:  t3_9e2bkb\n",
      "Print 'after' recieved:  t3_9dxft7\n",
      "Print 'after' recieved:  t3_9dxbnd\n",
      "Print 'after' recieved:  t3_9dykuq\n",
      "Print 'after' recieved:  t3_9dnt5r\n",
      "Print 'after' recieved:  t3_9dw440\n",
      "Print 'after' recieved:  t3_9dzsfi\n",
      "Print 'after' recieved:  t3_9dnct8\n",
      "Print 'after' recieved:  t3_9dl7ky\n",
      "Print 'after' recieved:  t3_9dpq1h\n",
      "Print 'after' recieved:  t3_9defb4\n",
      "Print 'after' recieved:  None\n"
     ]
    }
   ],
   "source": [
    "#Scraping from the nba subreddit\n",
    "#as it is in batches of 25, I made the number of posts 40 or over\n",
    "nba_subreddit=redditscraper(url='http://www.reddit.com/r/nba.json',number_of_posts = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print 'after' recieved:  t3_9ex64d\n",
      "Print 'after' recieved:  t3_9ezob5\n",
      "Print 'after' recieved:  t3_9eqkpu\n",
      "Print 'after' recieved:  t3_9eokf0\n",
      "Print 'after' recieved:  t3_9est22\n",
      "Print 'after' recieved:  t3_9egnum\n",
      "Print 'after' recieved:  t3_9eudxm\n",
      "Print 'after' recieved:  t3_9edyon\n",
      "Print 'after' recieved:  t3_9e6x41\n",
      "Print 'after' recieved:  t3_9dzaf9\n",
      "Print 'after' recieved:  t3_9dspwi\n",
      "Print 'after' recieved:  t3_9dxqi9\n",
      "Print 'after' recieved:  t3_9e2qfu\n",
      "Print 'after' recieved:  t3_9dz2tt\n",
      "Print 'after' recieved:  t3_9e1tff\n",
      "Print 'after' recieved:  t3_9dyqpg\n",
      "Print 'after' recieved:  t3_9dpiq4\n",
      "Print 'after' recieved:  t3_9dwpl1\n",
      "Print 'after' recieved:  t3_9dnfet\n",
      "Print 'after' recieved:  t3_9djjcb\n",
      "Print 'after' recieved:  t3_9dmpsx\n",
      "Print 'after' recieved:  t3_9di8hj\n",
      "Print 'after' recieved:  t3_9dkeme\n",
      "Print 'after' recieved:  t3_9dm2mp\n",
      "Print 'after' recieved:  t3_9dmxlu\n",
      "Print 'after' recieved:  t3_9dj0r1\n",
      "Print 'after' recieved:  t3_9di3z0\n",
      "Print 'after' recieved:  t3_9dm8oj\n",
      "Print 'after' recieved:  t3_9dkyc3\n",
      "Print 'after' recieved:  t3_9djb1n\n",
      "Print 'after' recieved:  t3_9d7seo\n",
      "Print 'after' recieved:  t3_9de119\n",
      "Print 'after' recieved:  t3_9ddtzw\n",
      "Print 'after' recieved:  t3_9d7ljz\n",
      "Print 'after' recieved:  t3_9d6qxj\n",
      "Print 'after' recieved:  None\n"
     ]
    }
   ],
   "source": [
    "#scraping from the wallstreetbets subreddit\n",
    "\n",
    "wallstreetbets_subreddit=redditscraper(url='http://www.reddit.com/r/wallstreetbets.json',number_of_posts = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'link_flair_text', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'parent_whitelist_status', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'author_flair_background_color', 'subreddit_type', 'ups', 'domain', 'media_embed', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'saved', 'can_mod_post', 'score', 'approved_by', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'banned_by', 'author_flair_type', 'contest_mode', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'media_only', 'link_flair_template_id', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'visited', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'report_reasons', 'author', 'num_crossposts', 'num_comments', 'send_replies', 'mod_reports', 'author_flair_text_color', 'permalink', 'whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'media', 'is_video'])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_subreddit[0]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'author_flair_background_color', 'subreddit_type', 'ups', 'domain', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'post_hint', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'banned_by', 'author_flair_type', 'contest_mode', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'preview', 'media_only', 'link_flair_template_id', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'visited', 'num_reports', 'distinguished', 'subreddit_id', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'report_reasons', 'author', 'num_crossposts', 'num_comments', 'send_replies', 'whitelist_status', 'mod_reports', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'media', 'is_video'])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallstreetbets_subreddit[0]['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into a dataframe using list comprehension\n",
    "nba = pd.DataFrame(x['data'] for x in nba_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td>Daily Locker Room and Free Talk + Game Threads...</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/9exfie/d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Chancelor_West</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Celtics4</td>\n",
       "      <td>[]</td>\n",
       "      <td>1b704ed0-362b-11e8-86da-0e69a4981634</td>\n",
       "      <td>Jabari Birdman</td>\n",
       "      <td>...</td>\n",
       "      <td>qa</td>\n",
       "      <td></td>\n",
       "      <td>[Announcement] Julie Phayer will be joining us...</td>\n",
       "      <td>93</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/9eqyxp/a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>flimsyfresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Lakers2</td>\n",
       "      <td>[]</td>\n",
       "      <td>3c4644d4-362b-11e8-b8e6-0e22df847c30</td>\n",
       "      <td>Lakers</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>My Uber driver had NBA Jam hooked up for passe...</td>\n",
       "      <td>14181</td>\n",
       "      <td>https://np.imgur.com/A1aWu27.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Ih8reposts</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>76ers3</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[PHI] Tiago Splitter</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Fun Fact: Andre Drummond has the most Offensiv...</td>\n",
       "      <td>681</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/9ey2vf/f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Mind_Fcuk</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Bullets</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Bullets</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Kyrie Irving is enrolled in a program at Harva...</td>\n",
       "      <td>252</td>\n",
       "      <td>https://www.boston.com/sports/boston-celtics/2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>KSmooove</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Lakers1</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Lakers</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Gilbert Arenas puts up $100k for shootout vs N...</td>\n",
       "      <td>155</td>\n",
       "      <td>https://www.youtube.com/watch?v=fZf0Gx4-5tU</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>gnullify</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Kings3</td>\n",
       "      <td>[]</td>\n",
       "      <td>deafde6e-3feb-11e8-89dc-0e993ebc6d5c</td>\n",
       "      <td>Kings</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Jaylen Brown hooping in T-Mac's INSANE home gym</td>\n",
       "      <td>464</td>\n",
       "      <td>https://youtu.be/1wViFt76cmk</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>scooper1030</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Suns1</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Suns</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>[OC] Making the case for TJ Warren as the NBA'...</td>\n",
       "      <td>176</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/9eyxpi/o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>vvvdvvv</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Spurs2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>What's the jersey you bought that you regrette...</td>\n",
       "      <td>240</td>\n",
       "      <td>https://www.reddit.com/r/nba/comments/9ex7i9/w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Mebegilley</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Pacers2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Pacers</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Kevin Garnett 32 Points, 21 Rebs, 5 Blocks 200...</td>\n",
       "      <td>89</td>\n",
       "      <td>https://www.youtube.com/watch?v=Vo4eqnG-lIQ</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc approved_by  archived          author author_cakeday  \\\n",
       "0            None        None     False   AutoModerator            NaN   \n",
       "1            None        None     False  Chancelor_West            NaN   \n",
       "2            None        None     False     flimsyfresh            NaN   \n",
       "3            None        None     False      Ih8reposts            NaN   \n",
       "4            None        None     False       Mind_Fcuk            NaN   \n",
       "5            None        None     False        KSmooove            NaN   \n",
       "6            None        None     False        gnullify            NaN   \n",
       "7            None        None     False     scooper1030            NaN   \n",
       "8            None        None     False         vvvdvvv            NaN   \n",
       "9            None        None     False      Mebegilley            NaN   \n",
       "\n",
       "  author_flair_background_color author_flair_css_class author_flair_richtext  \\\n",
       "0                          None                   None                    []   \n",
       "1                                             Celtics4                    []   \n",
       "2                                              Lakers2                    []   \n",
       "3                                               76ers3                    []   \n",
       "4                                              Bullets                    []   \n",
       "5                                              Lakers1                    []   \n",
       "6                                               Kings3                    []   \n",
       "7                                                Suns1                    []   \n",
       "8                                               Spurs2                    []   \n",
       "9                                              Pacers2                    []   \n",
       "\n",
       "               author_flair_template_id     author_flair_text ...  \\\n",
       "0                                  None                  None ...   \n",
       "1  1b704ed0-362b-11e8-86da-0e69a4981634        Jabari Birdman ...   \n",
       "2  3c4644d4-362b-11e8-b8e6-0e22df847c30                Lakers ...   \n",
       "3                                  None  [PHI] Tiago Splitter ...   \n",
       "4                                  None               Bullets ...   \n",
       "5                                  None                Lakers ...   \n",
       "6  deafde6e-3feb-11e8-89dc-0e993ebc6d5c                 Kings ...   \n",
       "7                                  None                  Suns ...   \n",
       "8                                  None                 Spurs ...   \n",
       "9                                  None                Pacers ...   \n",
       "\n",
       "  suggested_sort thumbnail                                              title  \\\n",
       "0            new            Daily Locker Room and Free Talk + Game Threads...   \n",
       "1             qa            [Announcement] Julie Phayer will be joining us...   \n",
       "2           None            My Uber driver had NBA Jam hooked up for passe...   \n",
       "3           None            Fun Fact: Andre Drummond has the most Offensiv...   \n",
       "4           None            Kyrie Irving is enrolled in a program at Harva...   \n",
       "5           None            Gilbert Arenas puts up $100k for shootout vs N...   \n",
       "6           None              Jaylen Brown hooping in T-Mac's INSANE home gym   \n",
       "7           None            [OC] Making the case for TJ Warren as the NBA'...   \n",
       "8           None            What's the jersey you bought that you regrette...   \n",
       "9           None            Kevin Garnett 32 Points, 21 Rebs, 5 Blocks 200...   \n",
       "\n",
       "     ups                                                url  user_reports  \\\n",
       "0     16  https://www.reddit.com/r/nba/comments/9exfie/d...            []   \n",
       "1     93  https://www.reddit.com/r/nba/comments/9eqyxp/a...            []   \n",
       "2  14181                   https://np.imgur.com/A1aWu27.jpg            []   \n",
       "3    681  https://www.reddit.com/r/nba/comments/9ey2vf/f...            []   \n",
       "4    252  https://www.boston.com/sports/boston-celtics/2...            []   \n",
       "5    155        https://www.youtube.com/watch?v=fZf0Gx4-5tU            []   \n",
       "6    464                       https://youtu.be/1wViFt76cmk            []   \n",
       "7    176  https://www.reddit.com/r/nba/comments/9eyxpi/o...            []   \n",
       "8    240  https://www.reddit.com/r/nba/comments/9ex7i9/w...            []   \n",
       "9     89        https://www.youtube.com/watch?v=Vo4eqnG-lIQ            []   \n",
       "\n",
       "   view_count visited  whitelist_status wls  \n",
       "0        None   False           all_ads   6  \n",
       "1        None   False           all_ads   6  \n",
       "2        None   False           all_ads   6  \n",
       "3        None   False           all_ads   6  \n",
       "4        None   False           all_ads   6  \n",
       "5        None   False           all_ads   6  \n",
       "6        None   False           all_ads   6  \n",
       "7        None   False           all_ads   6  \n",
       "8        None   False           all_ads   6  \n",
       "9        None   False           all_ads   6  \n",
       "\n",
       "[10 rows x 93 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = pd.DataFrame(x['data'] for x in wallstreetbets_subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What Are Your Moves Tomorrow, September 12</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>jarredshere</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Coming this Fall, The AMD Story</td>\n",
       "      <td>3554</td>\n",
       "      <td>https://i.redd.it/tkwzgwrc4ml11.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>WrennAmethyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Any emoji pattern experts in here?</td>\n",
       "      <td>239</td>\n",
       "      <td>https://i.imgur.com/a5fr5G5.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>kiLo28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>We made it boyos, 100% club.</td>\n",
       "      <td>192</td>\n",
       "      <td>https://i.redd.it/mvxnjqsywml11.png</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CANT_MILK_THOSE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Summary of Cramer's Micron Analysis these last...</td>\n",
       "      <td>184</td>\n",
       "      <td>https://i.redd.it/rejhxgqkqml11.png</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>IOutsourced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>EA Holders watching TTWO and ATVI today</td>\n",
       "      <td>96</td>\n",
       "      <td>https://i.redd.it/sp1rq9bornl11.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>POCKET_POOL_CHAMP</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': '⭐'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>⭐</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>$MSFT making my micro hard</td>\n",
       "      <td>214</td>\n",
       "      <td>https://i.redd.it/u1gvwlveeml11.png</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>GreatTraderOnizuka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NVDA $48k put follow up, closed</td>\n",
       "      <td>160</td>\n",
       "      <td>https://i.redd.it/g60qi7b0hml11.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>jeffynihao</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'Anyong Haseyo!'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>Anyong Haseyo!</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I sold SQ, ROKU, and AMD for MU 3 months ago</td>\n",
       "      <td>116</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>WSBConsensus</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[{'e': 'text', 't': 'cannot properly use flair'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>cannot properly use flair</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85% of insured Amazon (AMZN) Prime memebers op...</td>\n",
       "      <td>75</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>house_only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc approved_by  archived              author author_cakeday  \\\n",
       "0            None        None     False       AutoModerator            NaN   \n",
       "1            None        None     False         jarredshere            NaN   \n",
       "2            None        None     False       WrennAmethyst            NaN   \n",
       "3            None        None     False              kiLo28            NaN   \n",
       "4            None        None     False     CANT_MILK_THOSE            NaN   \n",
       "5            None        None     False         IOutsourced            NaN   \n",
       "6            None        None     False   POCKET_POOL_CHAMP            NaN   \n",
       "7            None        None     False  GreatTraderOnizuka            NaN   \n",
       "8            None        None     False          jeffynihao            NaN   \n",
       "9            None        None     False        WSBConsensus            NaN   \n",
       "\n",
       "  author_flair_background_color author_flair_css_class  \\\n",
       "0                          None                   None   \n",
       "1                          None                   None   \n",
       "2                          None                   None   \n",
       "3                          None                   None   \n",
       "4                          None                   None   \n",
       "5                          None                   None   \n",
       "6                                                 None   \n",
       "7                          None                   None   \n",
       "8                                                 None   \n",
       "9                                                 None   \n",
       "\n",
       "                               author_flair_richtext author_flair_template_id  \\\n",
       "0                                                 []                     None   \n",
       "1                                                 []                     None   \n",
       "2                                                 []                     None   \n",
       "3                                                 []                     None   \n",
       "4                                                 []                     None   \n",
       "5                                                 []                     None   \n",
       "6                          [{'e': 'text', 't': '⭐'}]                     None   \n",
       "7                                                 []                     None   \n",
       "8             [{'e': 'text', 't': 'Anyong Haseyo!'}]                     None   \n",
       "9  [{'e': 'text', 't': 'cannot properly use flair'}]                     None   \n",
       "\n",
       "           author_flair_text ... thumbnail_height thumbnail_width  \\\n",
       "0                       None ...              NaN             NaN   \n",
       "1                       None ...            140.0           140.0   \n",
       "2                       None ...            114.0           140.0   \n",
       "3                       None ...             87.0           140.0   \n",
       "4                       None ...            120.0           140.0   \n",
       "5                       None ...             73.0           140.0   \n",
       "6                          ⭐ ...             24.0           140.0   \n",
       "7                       None ...            140.0           140.0   \n",
       "8             Anyong Haseyo! ...              NaN             NaN   \n",
       "9  cannot properly use flair ...              NaN             NaN   \n",
       "\n",
       "                                               title   ups  \\\n",
       "0         What Are Your Moves Tomorrow, September 12     8   \n",
       "1                    Coming this Fall, The AMD Story  3554   \n",
       "2                 Any emoji pattern experts in here?   239   \n",
       "3                       We made it boyos, 100% club.   192   \n",
       "4  Summary of Cramer's Micron Analysis these last...   184   \n",
       "5            EA Holders watching TTWO and ATVI today    96   \n",
       "6                         $MSFT making my micro hard   214   \n",
       "7                    NVDA $48k put follow up, closed   160   \n",
       "8       I sold SQ, ROKU, and AMD for MU 3 months ago   116   \n",
       "9  85% of insured Amazon (AMZN) Prime memebers op...    75   \n",
       "\n",
       "                                                 url  user_reports  \\\n",
       "0  https://www.reddit.com/r/wallstreetbets/commen...            []   \n",
       "1                https://i.redd.it/tkwzgwrc4ml11.jpg            []   \n",
       "2                    https://i.imgur.com/a5fr5G5.jpg            []   \n",
       "3                https://i.redd.it/mvxnjqsywml11.png            []   \n",
       "4                https://i.redd.it/rejhxgqkqml11.png            []   \n",
       "5                https://i.redd.it/sp1rq9bornl11.jpg            []   \n",
       "6                https://i.redd.it/u1gvwlveeml11.png            []   \n",
       "7                https://i.redd.it/g60qi7b0hml11.jpg            []   \n",
       "8  https://www.reddit.com/r/wallstreetbets/commen...            []   \n",
       "9  https://www.reddit.com/r/wallstreetbets/commen...            []   \n",
       "\n",
       "   view_count visited  whitelist_status wls  \n",
       "0        None   False        house_only   1  \n",
       "1        None   False        house_only   1  \n",
       "2        None   False        house_only   1  \n",
       "3        None   False        house_only   1  \n",
       "4        None   False        house_only   1  \n",
       "5        None   False        house_only   1  \n",
       "6        None   False        house_only   1  \n",
       "7        None   False        house_only   1  \n",
       "8        None   False        house_only   1  \n",
       "9        None   False        house_only   1  \n",
       "\n",
       "[10 rows x 95 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsb.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Using frames to concat 2 dataframes\n",
    "frames = [nba,wsb]\n",
    "combined_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 nba\n",
       "1                 nba\n",
       "2                 nba\n",
       "3                 nba\n",
       "4                 nba\n",
       "5                 nba\n",
       "6                 nba\n",
       "7                 nba\n",
       "8                 nba\n",
       "9                 nba\n",
       "10                nba\n",
       "11                nba\n",
       "12                nba\n",
       "13                nba\n",
       "14                nba\n",
       "15                nba\n",
       "16                nba\n",
       "17                nba\n",
       "18                nba\n",
       "19                nba\n",
       "20                nba\n",
       "21                nba\n",
       "22                nba\n",
       "23                nba\n",
       "24                nba\n",
       "25                nba\n",
       "26                nba\n",
       "27                nba\n",
       "28                nba\n",
       "29                nba\n",
       "            ...      \n",
       "856    wallstreetbets\n",
       "857    wallstreetbets\n",
       "858    wallstreetbets\n",
       "859    wallstreetbets\n",
       "860    wallstreetbets\n",
       "861    wallstreetbets\n",
       "862    wallstreetbets\n",
       "863    wallstreetbets\n",
       "864    wallstreetbets\n",
       "865    wallstreetbets\n",
       "866    wallstreetbets\n",
       "867    wallstreetbets\n",
       "868    wallstreetbets\n",
       "869    wallstreetbets\n",
       "870    wallstreetbets\n",
       "871    wallstreetbets\n",
       "872    wallstreetbets\n",
       "873    wallstreetbets\n",
       "874    wallstreetbets\n",
       "875    wallstreetbets\n",
       "876    wallstreetbets\n",
       "877    wallstreetbets\n",
       "878    wallstreetbets\n",
       "879    wallstreetbets\n",
       "880    wallstreetbets\n",
       "881    wallstreetbets\n",
       "882    wallstreetbets\n",
       "883    wallstreetbets\n",
       "884    wallstreetbets\n",
       "885    wallstreetbets\n",
       "Name: subreddit, Length: 1616, dtype: object"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling the subreddit column in the combined dataframe to identify which post is in which subreddit\n",
    "combined_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 95)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 93)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616, 97)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "combined_df.to_csv('combined_DF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Daily Locker Room and Free Talk + Game Threads...\n",
       "1      [Announcement] Julie Phayer will be joining us...\n",
       "2      My Uber driver had NBA Jam hooked up for passe...\n",
       "3      Fun Fact: Andre Drummond has the most Offensiv...\n",
       "4      Kyrie Irving is enrolled in a program at Harva...\n",
       "5      Gilbert Arenas puts up $100k for shootout vs N...\n",
       "6        Jaylen Brown hooping in T-Mac's INSANE home gym\n",
       "7      [OC] Making the case for TJ Warren as the NBA'...\n",
       "8      What's the jersey you bought that you regrette...\n",
       "9      Kevin Garnett 32 Points, 21 Rebs, 5 Blocks 200...\n",
       "10     What are your favourites NBA urban legends and...\n",
       "11     Melo and Harden working on their chemistry in ...\n",
       "12                 Counting down 50-31 on the SI Top 100\n",
       "13     [Charania] Free agent Luol Deng has reached ag...\n",
       "14     Danny Green on Kahwi's future: \"The city of To...\n",
       "15     Possible new OKC Thunder city edition jersey. ...\n",
       "16     CJ commented “Carrrrrrrrrryyyyyyyy” and Jaylen...\n",
       "17     [OC] Check out my 2019 NBA draft prospect prof...\n",
       "18     Marco Belinelli is seriously the pride of his ...\n",
       "19     Dang, the Timberwolves can now trot out a line...\n",
       "20     Kevin Love must revert to Minnesota days if Ca...\n",
       "21     Steve Nash was 1% FG from averaging 50/40/90 f...\n",
       "22     Kyrie Irving, Kevin Durant, and James Harden s...\n",
       "23     Carmelo 38 points vs. Rookie KD with 37 points...\n",
       "24     Kemba Walker 46 points in 28 minutes (Grizzlie...\n",
       "25      Draymond Green cleaning the Splash Cousins' pool\n",
       "26     HOF Steve Nash decided retire at the perfect t...\n",
       "27     [Bobby Portis] Sources: Joakim Noah to the Tim...\n",
       "28     [Wojnarowski] Toronto is signing guard Kyle Co...\n",
       "29     “Press Conference Kobe” is one of my favorite ...\n",
       "                             ...                        \n",
       "856                 $SQ looks so tempting after that dip\n",
       "857                                      $AXP puts, bois\n",
       "858    Is this why social media stocks are falling to...\n",
       "859                              So you want to long $FB\n",
       "860                                   CRON update thread\n",
       "861                   All in IRBT. Get it while its hot.\n",
       "862                         Considering the Nike boycott\n",
       "863                                     Better than $AMD\n",
       "864    WSB, let's have a conversation. What is your r...\n",
       "865    Tilray- the Rocket that Won’t Stop, or Will it...\n",
       "866    I got this at Wendy's last night and went +5k ...\n",
       "867    Why Tesla is undervalued - DD: Mercedes admits...\n",
       "868                         Why do you guys hate Micron?\n",
       "869                                         AMD 9/14 30C\n",
       "870                                   Is this Socialism?\n",
       "871    Twitter &amp; Square down- reaction to Dorsey ...\n",
       "872    Who else here thinks the new Mercedes electric...\n",
       "873                 NFLX Calls, Let's see how this goes.\n",
       "874                                   perfectly weighted\n",
       "875    Put in market-orders of $AMD and $CRON calls p...\n",
       "876    Does the funding secured SEC investigation mea...\n",
       "877        Micron Technology, Inc. (MU) 49.38 (DOWN 5 %)\n",
       "878                         Anyone else playing NKE FD's\n",
       "879                 Why did you guys short china stocks?\n",
       "880                              I’m here for some karma\n",
       "881                      Is now finally the time for MU?\n",
       "882           Why do you buy calls at the all time high?\n",
       "883                                                Tlry?\n",
       "884                                          Short $AAPL\n",
       "885                      When the stock market is closed\n",
       "Name: title, Length: 1616, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the title to establish the difference as every post has a title that is relevant to the sub whereas selftext may vary\n",
    "combined_df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the subreddit names into 0's and 1's\n",
    "combined_df.subreddit = combined_df.subreddit.str.replace('wallstreetbets','1') \n",
    "combined_df.subreddit = combined_df.subreddit.str.replace('nba','0') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate y to the subreddit column of the combined dataframe\n",
    "y = combined_df.subreddit\n",
    "#I chose title as X as every reddit post has a title that is geared towards its respective subreddit discusion\n",
    "X = combined_df.title\n",
    "# Train test split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer and fitting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize using the number of features that you scrape\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "#fit and transform the train, but only transform the test\n",
    "X_train_counts = cvec.fit_transform(X_train)\n",
    "\n",
    "X_test_counts = cvec.transform(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score :\n",
      "0.9963031423290203\n",
      "Test Score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8838951310861424"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Logistic regression to fit and score for count vectorizer\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_counts, y_train)\n",
    "print(\"Train Score :\")\n",
    "print(log_reg.score(X_train_counts,y_train))\n",
    "print(\"Test Score:\")\n",
    "log_reg.score(X_test_counts, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf and fitting, with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same process for Tf-Idf as CountVec\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_tfid = tvec.fit_transform(X_train)\n",
    "X_test_tfid = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score :\n",
      "0.9926062846580407\n",
      "Train Score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.897003745318352"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#declare the model\n",
    "\n",
    "log_reg.fit(X_train_tfid, y_train)\n",
    "print(\"Test Score :\")\n",
    "print(log_reg.score(X_train_tfid,y_train))\n",
    "print(\"Train Score\")\n",
    "log_reg.score(X_test_tfid, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our score imporves when fitting the model with TF-Idf over CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mu</th>\n",
       "      <th>amd</th>\n",
       "      <th>calls</th>\n",
       "      <th>buy</th>\n",
       "      <th>short</th>\n",
       "      <th>tesla</th>\n",
       "      <th>week</th>\n",
       "      <th>tsla</th>\n",
       "      <th>long</th>\n",
       "      <th>puts</th>\n",
       "      <th>...</th>\n",
       "      <th>allen</th>\n",
       "      <th>james</th>\n",
       "      <th>basketball</th>\n",
       "      <th>season</th>\n",
       "      <th>players</th>\n",
       "      <th>game</th>\n",
       "      <th>player</th>\n",
       "      <th>lebron</th>\n",
       "      <th>team</th>\n",
       "      <th>nba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.494332</td>\n",
       "      <td>2.147041</td>\n",
       "      <td>1.733749</td>\n",
       "      <td>1.612342</td>\n",
       "      <td>1.498732</td>\n",
       "      <td>1.465826</td>\n",
       "      <td>1.429913</td>\n",
       "      <td>1.398775</td>\n",
       "      <td>1.391733</td>\n",
       "      <td>1.35384</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.196126</td>\n",
       "      <td>-1.206766</td>\n",
       "      <td>-1.260888</td>\n",
       "      <td>-1.393497</td>\n",
       "      <td>-1.807938</td>\n",
       "      <td>-1.887318</td>\n",
       "      <td>-2.00782</td>\n",
       "      <td>-2.013422</td>\n",
       "      <td>-2.377234</td>\n",
       "      <td>-3.869232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2996 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mu       amd     calls       buy     short     tesla      week  \\\n",
       "0  2.494332  2.147041  1.733749  1.612342  1.498732  1.465826  1.429913   \n",
       "\n",
       "       tsla      long     puts    ...        allen     james  basketball  \\\n",
       "0  1.398775  1.391733  1.35384    ...    -1.196126 -1.206766   -1.260888   \n",
       "\n",
       "     season   players      game   player    lebron      team       nba  \n",
       "0 -1.393497 -1.807938 -1.887318 -2.00782 -2.013422 -2.377234 -3.869232  \n",
       "\n",
       "[1 rows x 2996 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From the vectorizer we can aquire the words we want\n",
    "columns=cvec.get_feature_names()\n",
    "#Make a DF of the words\n",
    "co_ef = pd.DataFrame(log_reg.coef_, columns = columns)\n",
    "\n",
    "#Sort from most to least compatible with wallstreetbets\n",
    "df_coef = co_ef.T.sort_values(by=0, ascending=False).T\n",
    "\n",
    "df_coef.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This list helps us understand what words relate with each subreddit. Worlds like 'mu' 'amd' and 'tesla' have a strong correlation to r/wallstreetbets whereas words like 'game' 'players' 'lebron' 'nba' have a strong affinity with r/nba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "### Predicting subreddit using Random Forests and Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree-CountVec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:\n",
      "0.9972273567467652\n",
      "Test Score :\n",
      "0.8314606741573034\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier with CountVec Model\n",
    "decision_tree = DecisionTreeClassifier(max_features=30,random_state=33)\n",
    "#                                 ?\n",
    "decision_tree = decision_tree.fit(X_train_counts, y_train)\n",
    "\n",
    "print(\"Train Score:\")\n",
    "print(decision_tree.score(X_train_counts, y_train))\n",
    "print(\"Test Score :\")\n",
    "print(decision_tree.score(X_test_counts,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.72      0.80       255\n",
      "          1       0.78      0.94      0.85       279\n",
      "\n",
      "avg / total       0.84      0.83      0.83       534\n",
      "\n",
      "This is the Confusion Matrix\n",
      "[[183  72]\n",
      " [ 18 261]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix for Decision tree using the predictions from the CountVec Model\n",
    "predictions= decision_tree.predict(X_test_counts)\n",
    "print('This is the Classification Report')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('This is the Confusion Matrix')\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for CountVec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:\n",
      "0.8401109057301294\n",
      "Test Score :\n",
      "0.7734082397003745\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier with the CountVec Model, \n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=100, max_leaf_nodes=24, max_features=1000)\n",
    "                                \n",
    "rf = clf.fit(X_train_counts, y_train)\n",
    "\n",
    "print(\"Train Score:\")\n",
    "print(rf.score(X_train_counts, y_train))\n",
    "print(\"Test Score :\")\n",
    "print(rf.score(X_test_counts,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.53      0.69       255\n",
      "          1       0.70      0.99      0.82       279\n",
      "\n",
      "avg / total       0.84      0.77      0.76       534\n",
      "\n",
      "This is the Confusion Matrix\n",
      "[[136 119]\n",
      " [  2 277]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=100, max_features=1000, max_leaf_nodes=24,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions= rf.predict(X_test_counts)\n",
    "print('This is the Classification Report')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('This is the Confusion Matrix')\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "rf.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree for Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:\n",
      "1.0\n",
      "Test Score :\n",
      "0.8127340823970037\n"
     ]
    }
   ],
   "source": [
    "decision_tree1 = DecisionTreeClassifier(max_features=30,random_state=66)\n",
    "                              \n",
    "decision_tree1 = decision_tree1.fit(X_train_tfid, y_train)\n",
    "\n",
    "print(\"Train Score:\")\n",
    "print(decision_tree1.score(X_train_tfid, y_train))\n",
    "print(\"Test Score :\")\n",
    "print(decision_tree1.score(X_test_tfid,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest for Tf-Idf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:\n",
      "0.8345656192236599\n",
      "Test Score :\n",
      "0.7752808988764045\n"
     ]
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=10, max_depth=200, max_leaf_nodes=12, max_features=1000)\n",
    "                                 \n",
    "rf1 = rf.fit(X_train_tfid, y_train)\n",
    "\n",
    "print(\"Train Score:\")\n",
    "print(rf1.score(X_train_tfid, y_train))\n",
    "print(\"Test Score :\")\n",
    "print(rf1.score(X_test_tfid,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding thoughts and results are discussed in the Executive Summary in the README file"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
